{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, concatenate, regularizers, \\\n",
    "                          Conv2D, MaxPool2D, Flatten, Activation, GlobalAveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data_for_conv2D(X):\n",
    "    X_conv2D = []\n",
    "    for sample in X:\n",
    "        sample = np.reshape(sample, newshape=(1, sample.shape[0], 1))\n",
    "        X_conv2D.append(sample)\n",
    "    return np.array(X_conv2D, dtype=np.float32)\n",
    "\n",
    "def data_iter(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    idx = list(range(num_samples))\n",
    "    while True:\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            j = idx[i:min(i+batch_size, num_samples)]\n",
    "            yield X[j], y[j]\n",
    "            \n",
    "def cal_auc(y_true, y_predict_pre):\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_true, y_predict_pre)\n",
    "    roc_auc = metrics.auc(false_positive_rate, true_positive_rate)\n",
    "    return roc_auc\n",
    "\n",
    "def get_ks_score(label, prob):\n",
    "    prob = prob[:,0]\n",
    "    df = pd.DataFrame(data = {'label': list(label), 'prob': list(prob)})\n",
    "    df['prob'] = df['prob'].map(lambda x: round(x, 3))\n",
    "    total = pd.DataFrame({'total': df.groupby('prob')['label'].count()})\n",
    "    bad = pd.DataFrame({'bad': df.groupby('prob')['label'].sum()})\n",
    "    all_data = total.merge(bad, how = 'left', left_index = True, right_index = True)\n",
    "    all_data['good'] = all_data['total'] - all_data['bad']\n",
    "    all_data.reset_index(inplace = True)\n",
    "    all_data['goodCumPer'] = all_data['good'].cumsum() / all_data['good'].sum()\n",
    "    all_data['badCumPer'] = all_data['bad'].cumsum() / all_data['bad'].sum()\n",
    "    KS_m = all_data.apply(lambda x: x.goodCumPer - x.badCumPer, axis = 1)\n",
    "    return max(KS_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #def f(input_, wd):\n",
    "        #x = Dense(256, kernel_regularizer=regularizers.l2(wd))(input_)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = LeakyReLU(alpha=0.3)(x)\n",
    "        #x = Dropout(0.5)(x)\n",
    "        \n",
    "        #x = Dense(128, kernel_regularizer=regularizers.l2(wd))(x)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = LeakyReLU(alpha=0.3)(x)\n",
    "        #x = Dropout(0.5)(x)\n",
    "        \n",
    "        #x = Dense(64, kernel_regularizer=regularizers.l2(wd))(input_)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = LeakyReLU(alpha=0.3)(x)\n",
    "        #x = Dropout(0.5)(x)\n",
    "        \n",
    "        #x = Dense(32, kernel_regularizer=regularizers.l2(wd))(input_)\n",
    "        #x = BatchNormalization()(x)\n",
    "        #x = LeakyReLU(alpha=0.3)(x)\n",
    "       # x = Dropout(0.5)(x)\n",
    "      #  \n",
    "     #   model_output = Dense(1, activation='sigmoid')(x)\n",
    "    #return model_output\n",
    "\n",
    "def CNNBaseBlock(wd):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, kernel_regularizer=regularizers.l2(wd), input_dim = 284))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(128, kernel_regularizer=regularizers.l2(wd)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(64, kernel_regularizer=regularizers.l2(wd)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(32, kernel_regularizer=regularizers.l2(wd)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallelizedCNN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, kernel_regularizer= regularizers.l2(0.0001), activity_regularizer= regularizers.l2(0.0001), input_dim = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation= 'sigmoid'))\n",
    "    #model_input = Input(shape=input_shape)\n",
    "    \n",
    "    #x1 = CNNBaseBlock(0.0001)(model_input)\n",
    "    #x2 = CNNBaseBlock(0.0002)(model_input)\n",
    "    \n",
    "    #x_merged = concatenate([x1, x2])\n",
    "    \n",
    "    #x = Dense(8, kernel_regularizer=regularizers.l2(0.0001),\n",
    "                # activity_regularizer=regularizers.l2(0.0001))(x_merged)\n",
    "   # x = BatchNormalization()(x)\n",
    "    #x = LeakyReLU(alpha=0.3)(x)\n",
    "   # x = Dropout(0.5)(x)\n",
    "    \n",
    "   # model_output = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "   # model = Model(inputs=model_input, outputs=model_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "batch_size = 64\n",
    "opt = Adam(lr=0.001)\n",
    "class_weight = {1:1, 0:1}\n",
    "#param = { 'net' : CNNBaseBlock}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_AB(X_train, y_train, X_valid, y_valid, X_test, y_test, wd, param):\n",
    "    ## get data\n",
    "    train_data_iter = data_iter(X_train, y_train, batch_size)\n",
    "    model = CNNBaseBlock(wd)\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "\n",
    "    keep_indicator = -1\n",
    "    keep_valid_auc = 0; keep_test_auc = 0; keep_valid_ks = 0; keep_test_ks = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        batchs = 0\n",
    "        for X_batch, y_batch in train_data_iter:\n",
    "            model.train_on_batch(X_batch, y_batch, class_weight=class_weight)\n",
    "            batchs += 1\n",
    "            if batchs >= len(X_train) / (batch_size / 2):\n",
    "                break\n",
    "            \n",
    "        train_loss, train_acc = model.evaluate(x = X_train, y = y_train, batch_size = 64, verbose = 0)\n",
    "        valid_loss, valid_acc = model.evaluate(x = X_test, y = y_test, batch_size = 64, verbose = 0)\n",
    "        test_loss, test_acc = model.evaluate(x = X_test, y = y_test, batch_size = 64, verbose = 0)\n",
    "            \n",
    "        y_train_pre = model.predict(X_train)\n",
    "        y_valid_pre = model.predict(X_valid)\n",
    "        y_test_pre = model.predict(X_test)\n",
    "    \n",
    "        train_auc = cal_auc(y_train, y_train_pre)\n",
    "        valid_auc = cal_auc(y_valid, y_valid_pre)\n",
    "        test_auc = cal_auc(y_test, y_test_pre)\n",
    "    \n",
    "        train_ks = get_ks_score(y_train, y_train_pre)\n",
    "        valid_ks = get_ks_score(y_valid, y_valid_pre)\n",
    "        test_ks = get_ks_score(y_test, y_test_pre)\n",
    "        \n",
    "        \n",
    "        f1 = valid_auc * valid_ks / (valid_auc + valid_ks) \n",
    "        if f1 > keep_indicator:\n",
    "            keep_indicator = f1\n",
    "\n",
    "            keep_valid_auc = valid_auc\n",
    "            keep_valid_ks = valid_ks\n",
    "            keep_test_auc = test_auc\n",
    "            keep_test_ks = test_ks\n",
    "            \n",
    "            model.save(param ['params_model_path'])\n",
    "    \n",
    "        print('Ep %3d L: %.3f auc: %.3f ks: %.3f | '\n",
    "                    'L: %.3f auc: %.3f ks: %.3f | '\n",
    "                    'L: %.3f auc: %.3f ks: %.3f' % \n",
    "              (e+1, train_loss, train_auc, train_ks, valid_loss, valid_auc, valid_ks, test_loss, test_auc, test_ks))\n",
    "\n",
    "    print('valid_auc', keep_valid_auc)\n",
    "    print('valid_ks', keep_valid_ks)\n",
    "    print('test_auc', keep_test_auc)\n",
    "    print('test_ks', keep_test_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_2rd(X_train, y_train, X_valid, y_valid, X_test, y_test, param):\n",
    "    ## get data\n",
    "    train_data_iter = data_iter(X_train, y_train, batch_size)\n",
    "    model = parallelizedCNN()\n",
    "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "\n",
    "    keep_indicator = -1\n",
    "    keep_valid_auc = 0; keep_test_auc = 0; keep_valid_ks = 0; keep_test_ks = 0\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        batchs = 0\n",
    "        for X_batch, y_batch in train_data_iter:\n",
    "            model.train_on_batch(X_batch, y_batch, class_weight=class_weight)\n",
    "            batchs += 1\n",
    "            if batchs >= len(X_train) / (batch_size / 2):\n",
    "                break\n",
    "            \n",
    "        train_loss, train_acc = model.evaluate(x = X_train, y = y_train, batch_size = 64, verbose = 0)\n",
    "        valid_loss, valid_acc = model.evaluate(x = X_test, y = y_test, batch_size = 64, verbose = 0)\n",
    "        test_loss, test_acc = model.evaluate(x = X_test, y = y_test, batch_size = 64, verbose = 0)\n",
    "            \n",
    "        y_train_pre = model.predict(X_train)\n",
    "        y_valid_pre = model.predict(X_valid)\n",
    "        y_test_pre = model.predict(X_test)\n",
    "    \n",
    "        train_auc = cal_auc(y_train, y_train_pre)\n",
    "        valid_auc = cal_auc(y_valid, y_valid_pre)\n",
    "        test_auc = cal_auc(y_test, y_test_pre)\n",
    "    \n",
    "        train_ks = get_ks_score(y_train, y_train_pre)\n",
    "        valid_ks = get_ks_score(y_valid, y_valid_pre)\n",
    "        test_ks = get_ks_score(y_test, y_test_pre)\n",
    "        \n",
    "        \n",
    "        f1 = valid_auc * valid_ks / (valid_auc + valid_ks) \n",
    "        if f1 > keep_indicator:\n",
    "            keep_indicator = f1\n",
    "\n",
    "            keep_valid_auc = valid_auc\n",
    "            keep_valid_ks = valid_ks\n",
    "            keep_test_auc = test_auc\n",
    "            keep_test_ks = test_ks\n",
    "            \n",
    "            model.save(param ['params_model_path'])\n",
    "    \n",
    "        print('Ep %3d L: %.3f auc: %.3f ks: %.3f | '\n",
    "                    'L: %.3f auc: %.3f ks: %.3f | '\n",
    "                    'L: %.3f auc: %.3f ks: %.3f' % \n",
    "              (e+1, train_loss, train_auc, train_ks, valid_loss, valid_auc, valid_ks, test_loss, test_auc, test_ks))\n",
    "\n",
    "    print('valid_auc', keep_valid_auc)\n",
    "    print('valid_ks', keep_valid_ks)\n",
    "    print('test_auc', keep_test_auc)\n",
    "    print('test_ks', keep_test_ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataSplit(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.24, random_state = 0)\n",
    "    X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 0)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anacoda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training net A...\n",
      "Ep   1 L: 0.080 auc: 0.731 ks: 0.327 | L: 0.079 auc: 0.722 ks: 0.325 | L: 0.079 auc: 0.736 ks: 0.353\n",
      "Ep   2 L: 0.066 auc: 0.767 ks: 0.380 | L: 0.066 auc: 0.759 ks: 0.444 | L: 0.066 auc: 0.760 ks: 0.427\n",
      "Ep   3 L: 0.057 auc: 0.775 ks: 0.396 | L: 0.057 auc: 0.755 ks: 0.445 | L: 0.057 auc: 0.774 ks: 0.444\n",
      "Ep   4 L: 0.053 auc: 0.768 ks: 0.395 | L: 0.054 auc: 0.754 ks: 0.449 | L: 0.054 auc: 0.755 ks: 0.403\n",
      "Ep   5 L: 0.051 auc: 0.771 ks: 0.405 | L: 0.051 auc: 0.773 ks: 0.464 | L: 0.051 auc: 0.766 ks: 0.431\n",
      "Ep   6 L: 0.051 auc: 0.747 ks: 0.376 | L: 0.051 auc: 0.725 ks: 0.370 | L: 0.051 auc: 0.744 ks: 0.409\n",
      "Ep   7 L: 0.050 auc: 0.751 ks: 0.373 | L: 0.050 auc: 0.740 ks: 0.398 | L: 0.050 auc: 0.750 ks: 0.466\n",
      "Ep   8 L: 0.049 auc: 0.768 ks: 0.385 | L: 0.049 auc: 0.751 ks: 0.416 | L: 0.049 auc: 0.763 ks: 0.437\n",
      "Ep   9 L: 0.049 auc: 0.754 ks: 0.391 | L: 0.049 auc: 0.750 ks: 0.421 | L: 0.049 auc: 0.747 ks: 0.419\n",
      "Ep  10 L: 0.048 auc: 0.773 ks: 0.407 | L: 0.048 auc: 0.765 ks: 0.445 | L: 0.048 auc: 0.766 ks: 0.474\n",
      "Ep  11 L: 0.048 auc: 0.771 ks: 0.411 | L: 0.048 auc: 0.768 ks: 0.435 | L: 0.048 auc: 0.752 ks: 0.444\n",
      "Ep  12 L: 0.048 auc: 0.781 ks: 0.414 | L: 0.048 auc: 0.761 ks: 0.437 | L: 0.048 auc: 0.771 ks: 0.457\n",
      "Ep  13 L: 0.049 auc: 0.733 ks: 0.350 | L: 0.049 auc: 0.739 ks: 0.389 | L: 0.049 auc: 0.726 ks: 0.393\n",
      "Ep  14 L: 0.048 auc: 0.770 ks: 0.416 | L: 0.048 auc: 0.763 ks: 0.468 | L: 0.048 auc: 0.758 ks: 0.489\n",
      "Ep  15 L: 0.048 auc: 0.764 ks: 0.406 | L: 0.048 auc: 0.753 ks: 0.414 | L: 0.048 auc: 0.747 ks: 0.433\n",
      "Ep  16 L: 0.047 auc: 0.783 ks: 0.430 | L: 0.047 auc: 0.768 ks: 0.460 | L: 0.047 auc: 0.769 ks: 0.487\n",
      "Ep  17 L: 0.048 auc: 0.759 ks: 0.393 | L: 0.048 auc: 0.750 ks: 0.404 | L: 0.048 auc: 0.742 ks: 0.401\n",
      "Ep  18 L: 0.047 auc: 0.773 ks: 0.411 | L: 0.047 auc: 0.762 ks: 0.443 | L: 0.047 auc: 0.762 ks: 0.474\n",
      "Ep  19 L: 0.047 auc: 0.773 ks: 0.409 | L: 0.047 auc: 0.756 ks: 0.452 | L: 0.047 auc: 0.767 ks: 0.475\n",
      "Ep  20 L: 0.047 auc: 0.787 ks: 0.427 | L: 0.047 auc: 0.771 ks: 0.471 | L: 0.047 auc: 0.780 ks: 0.480\n",
      "Ep  21 L: 0.047 auc: 0.780 ks: 0.423 | L: 0.047 auc: 0.762 ks: 0.448 | L: 0.047 auc: 0.764 ks: 0.472\n",
      "Ep  22 L: 0.047 auc: 0.785 ks: 0.426 | L: 0.047 auc: 0.773 ks: 0.466 | L: 0.047 auc: 0.766 ks: 0.427\n",
      "Ep  23 L: 0.047 auc: 0.781 ks: 0.419 | L: 0.047 auc: 0.769 ks: 0.451 | L: 0.047 auc: 0.769 ks: 0.431\n",
      "Ep  24 L: 0.047 auc: 0.792 ks: 0.435 | L: 0.047 auc: 0.776 ks: 0.465 | L: 0.047 auc: 0.774 ks: 0.459\n",
      "Ep  25 L: 0.047 auc: 0.778 ks: 0.417 | L: 0.046 auc: 0.764 ks: 0.457 | L: 0.046 auc: 0.775 ks: 0.494\n",
      "Ep  26 L: 0.047 auc: 0.776 ks: 0.406 | L: 0.047 auc: 0.761 ks: 0.445 | L: 0.047 auc: 0.768 ks: 0.478\n",
      "Ep  27 L: 0.046 auc: 0.792 ks: 0.430 | L: 0.046 auc: 0.764 ks: 0.452 | L: 0.046 auc: 0.773 ks: 0.463\n",
      "Ep  28 L: 0.046 auc: 0.793 ks: 0.436 | L: 0.046 auc: 0.776 ks: 0.473 | L: 0.046 auc: 0.780 ks: 0.470\n",
      "Ep  29 L: 0.046 auc: 0.782 ks: 0.425 | L: 0.046 auc: 0.756 ks: 0.439 | L: 0.046 auc: 0.761 ks: 0.474\n",
      "Ep  30 L: 0.046 auc: 0.790 ks: 0.429 | L: 0.046 auc: 0.779 ks: 0.481 | L: 0.046 auc: 0.765 ks: 0.434\n",
      "Ep  31 L: 0.046 auc: 0.790 ks: 0.433 | L: 0.046 auc: 0.758 ks: 0.429 | L: 0.046 auc: 0.783 ks: 0.486\n",
      "Ep  32 L: 0.047 auc: 0.779 ks: 0.413 | L: 0.047 auc: 0.756 ks: 0.424 | L: 0.047 auc: 0.772 ks: 0.456\n",
      "Ep  33 L: 0.046 auc: 0.785 ks: 0.428 | L: 0.046 auc: 0.773 ks: 0.499 | L: 0.046 auc: 0.767 ks: 0.455\n",
      "Ep  34 L: 0.046 auc: 0.784 ks: 0.432 | L: 0.046 auc: 0.763 ks: 0.442 | L: 0.046 auc: 0.771 ks: 0.450\n",
      "Ep  35 L: 0.046 auc: 0.781 ks: 0.426 | L: 0.046 auc: 0.776 ks: 0.466 | L: 0.046 auc: 0.770 ks: 0.497\n",
      "Ep  36 L: 0.046 auc: 0.779 ks: 0.415 | L: 0.047 auc: 0.766 ks: 0.455 | L: 0.047 auc: 0.762 ks: 0.443\n",
      "Ep  37 L: 0.046 auc: 0.794 ks: 0.440 | L: 0.046 auc: 0.775 ks: 0.454 | L: 0.046 auc: 0.774 ks: 0.435\n",
      "Ep  38 L: 0.046 auc: 0.791 ks: 0.440 | L: 0.046 auc: 0.776 ks: 0.437 | L: 0.046 auc: 0.767 ks: 0.439\n",
      "Ep  39 L: 0.046 auc: 0.791 ks: 0.436 | L: 0.046 auc: 0.774 ks: 0.477 | L: 0.046 auc: 0.775 ks: 0.485\n",
      "Ep  40 L: 0.046 auc: 0.794 ks: 0.437 | L: 0.046 auc: 0.769 ks: 0.458 | L: 0.046 auc: 0.779 ks: 0.462\n",
      "valid_auc 0.77291642892\n",
      "valid_ks 0.498721353653\n",
      "test_auc 0.767348365313\n",
      "test_ks 0.454890434963\n",
      "load the model A....\n",
      "training net B...\n",
      "Ep   1 L: 0.070 auc: 0.757 ks: 0.364 | L: 0.070 auc: 0.760 ks: 0.449 | L: 0.070 auc: 0.764 ks: 0.434\n",
      "Ep   2 L: 0.058 auc: 0.768 ks: 0.389 | L: 0.058 auc: 0.763 ks: 0.449 | L: 0.058 auc: 0.750 ks: 0.420\n",
      "Ep   3 L: 0.054 auc: 0.762 ks: 0.386 | L: 0.055 auc: 0.752 ks: 0.423 | L: 0.055 auc: 0.747 ks: 0.394\n",
      "Ep   4 L: 0.053 auc: 0.752 ks: 0.376 | L: 0.053 auc: 0.744 ks: 0.415 | L: 0.053 auc: 0.751 ks: 0.403\n",
      "Ep   5 L: 0.052 auc: 0.758 ks: 0.390 | L: 0.052 auc: 0.767 ks: 0.462 | L: 0.052 auc: 0.747 ks: 0.459\n",
      "Ep   6 L: 0.051 auc: 0.769 ks: 0.391 | L: 0.051 auc: 0.753 ks: 0.451 | L: 0.051 auc: 0.762 ks: 0.447\n",
      "Ep   7 L: 0.050 auc: 0.758 ks: 0.376 | L: 0.050 auc: 0.753 ks: 0.443 | L: 0.050 auc: 0.755 ks: 0.464\n",
      "Ep   8 L: 0.050 auc: 0.764 ks: 0.403 | L: 0.050 auc: 0.770 ks: 0.459 | L: 0.050 auc: 0.752 ks: 0.458\n",
      "Ep   9 L: 0.050 auc: 0.762 ks: 0.398 | L: 0.050 auc: 0.773 ks: 0.464 | L: 0.050 auc: 0.742 ks: 0.442\n",
      "Ep  10 L: 0.050 auc: 0.728 ks: 0.350 | L: 0.050 auc: 0.738 ks: 0.375 | L: 0.050 auc: 0.725 ks: 0.373\n",
      "Ep  11 L: 0.050 auc: 0.745 ks: 0.369 | L: 0.050 auc: 0.757 ks: 0.419 | L: 0.050 auc: 0.734 ks: 0.443\n",
      "Ep  12 L: 0.049 auc: 0.758 ks: 0.384 | L: 0.049 auc: 0.768 ks: 0.438 | L: 0.049 auc: 0.745 ks: 0.430\n",
      "Ep  13 L: 0.048 auc: 0.782 ks: 0.417 | L: 0.049 auc: 0.776 ks: 0.470 | L: 0.049 auc: 0.765 ks: 0.439\n",
      "Ep  14 L: 0.049 auc: 0.760 ks: 0.387 | L: 0.049 auc: 0.759 ks: 0.440 | L: 0.049 auc: 0.756 ks: 0.446\n",
      "Ep  15 L: 0.049 auc: 0.758 ks: 0.389 | L: 0.048 auc: 0.762 ks: 0.419 | L: 0.048 auc: 0.749 ks: 0.455\n",
      "Ep  16 L: 0.049 auc: 0.748 ks: 0.363 | L: 0.049 auc: 0.745 ks: 0.408 | L: 0.049 auc: 0.736 ks: 0.386\n",
      "Ep  17 L: 0.049 auc: 0.773 ks: 0.417 | L: 0.049 auc: 0.773 ks: 0.466 | L: 0.049 auc: 0.761 ks: 0.475\n",
      "Ep  18 L: 0.048 auc: 0.767 ks: 0.402 | L: 0.048 auc: 0.762 ks: 0.450 | L: 0.048 auc: 0.760 ks: 0.470\n",
      "Ep  19 L: 0.048 auc: 0.775 ks: 0.413 | L: 0.048 auc: 0.773 ks: 0.462 | L: 0.048 auc: 0.760 ks: 0.460\n",
      "Ep  20 L: 0.048 auc: 0.772 ks: 0.400 | L: 0.048 auc: 0.761 ks: 0.441 | L: 0.048 auc: 0.766 ks: 0.448\n",
      "Ep  21 L: 0.047 auc: 0.778 ks: 0.410 | L: 0.047 auc: 0.763 ks: 0.458 | L: 0.047 auc: 0.773 ks: 0.488\n",
      "Ep  22 L: 0.048 auc: 0.768 ks: 0.402 | L: 0.048 auc: 0.775 ks: 0.467 | L: 0.048 auc: 0.755 ks: 0.429\n",
      "Ep  23 L: 0.047 auc: 0.775 ks: 0.404 | L: 0.047 auc: 0.761 ks: 0.446 | L: 0.047 auc: 0.773 ks: 0.469\n",
      "Ep  24 L: 0.047 auc: 0.762 ks: 0.395 | L: 0.047 auc: 0.760 ks: 0.453 | L: 0.047 auc: 0.758 ks: 0.470\n",
      "Ep  25 L: 0.047 auc: 0.781 ks: 0.415 | L: 0.047 auc: 0.772 ks: 0.445 | L: 0.047 auc: 0.758 ks: 0.420\n",
      "Ep  26 L: 0.047 auc: 0.781 ks: 0.423 | L: 0.047 auc: 0.761 ks: 0.432 | L: 0.047 auc: 0.782 ks: 0.476\n",
      "Ep  27 L: 0.047 auc: 0.771 ks: 0.418 | L: 0.047 auc: 0.767 ks: 0.453 | L: 0.047 auc: 0.767 ks: 0.456\n",
      "Ep  28 L: 0.047 auc: 0.762 ks: 0.397 | L: 0.048 auc: 0.756 ks: 0.412 | L: 0.048 auc: 0.746 ks: 0.429\n",
      "Ep  29 L: 0.047 auc: 0.787 ks: 0.425 | L: 0.047 auc: 0.777 ks: 0.465 | L: 0.047 auc: 0.774 ks: 0.475\n",
      "Ep  30 L: 0.047 auc: 0.776 ks: 0.420 | L: 0.047 auc: 0.765 ks: 0.460 | L: 0.047 auc: 0.769 ks: 0.486\n",
      "Ep  31 L: 0.047 auc: 0.786 ks: 0.423 | L: 0.047 auc: 0.770 ks: 0.471 | L: 0.047 auc: 0.780 ks: 0.455\n",
      "Ep  32 L: 0.047 auc: 0.775 ks: 0.422 | L: 0.047 auc: 0.774 ks: 0.479 | L: 0.047 auc: 0.763 ks: 0.471\n",
      "Ep  33 L: 0.047 auc: 0.789 ks: 0.434 | L: 0.047 auc: 0.774 ks: 0.463 | L: 0.047 auc: 0.769 ks: 0.432\n",
      "Ep  34 L: 0.047 auc: 0.769 ks: 0.416 | L: 0.047 auc: 0.750 ks: 0.421 | L: 0.047 auc: 0.762 ks: 0.448\n",
      "Ep  35 L: 0.047 auc: 0.780 ks: 0.407 | L: 0.047 auc: 0.767 ks: 0.452 | L: 0.047 auc: 0.771 ks: 0.492\n",
      "Ep  36 L: 0.046 auc: 0.785 ks: 0.428 | L: 0.047 auc: 0.779 ks: 0.463 | L: 0.047 auc: 0.768 ks: 0.448\n",
      "Ep  37 L: 0.047 auc: 0.790 ks: 0.429 | L: 0.046 auc: 0.771 ks: 0.495 | L: 0.046 auc: 0.787 ks: 0.468\n",
      "Ep  38 L: 0.047 auc: 0.775 ks: 0.414 | L: 0.047 auc: 0.774 ks: 0.477 | L: 0.047 auc: 0.765 ks: 0.457\n",
      "Ep  39 L: 0.046 auc: 0.783 ks: 0.427 | L: 0.046 auc: 0.772 ks: 0.469 | L: 0.046 auc: 0.776 ks: 0.470\n",
      "Ep  40 L: 0.047 auc: 0.772 ks: 0.414 | L: 0.047 auc: 0.757 ks: 0.432 | L: 0.047 auc: 0.762 ks: 0.475\n",
      "valid_auc 0.770869427644\n",
      "valid_ks 0.495323733544\n",
      "test_auc 0.786827805347\n",
      "test_ks 0.467817229003\n",
      "load the model B....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing the 2rd net...\n",
      "Ep   1 L: 0.046 auc: 0.787 ks: 0.429 | L: 0.046 auc: 0.773 ks: 0.446 | L: 0.046 auc: 0.772 ks: 0.429\n",
      "Ep   2 L: 0.045 auc: 0.787 ks: 0.432 | L: 0.046 auc: 0.773 ks: 0.468 | L: 0.046 auc: 0.774 ks: 0.432\n",
      "Ep   3 L: 0.045 auc: 0.788 ks: 0.428 | L: 0.046 auc: 0.773 ks: 0.482 | L: 0.046 auc: 0.775 ks: 0.440\n",
      "Ep   4 L: 0.045 auc: 0.788 ks: 0.429 | L: 0.045 auc: 0.773 ks: 0.474 | L: 0.045 auc: 0.776 ks: 0.451\n",
      "Ep   5 L: 0.045 auc: 0.788 ks: 0.425 | L: 0.045 auc: 0.773 ks: 0.473 | L: 0.045 auc: 0.777 ks: 0.448\n",
      "Ep   6 L: 0.045 auc: 0.788 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.479 | L: 0.045 auc: 0.778 ks: 0.462\n",
      "Ep   7 L: 0.045 auc: 0.789 ks: 0.431 | L: 0.045 auc: 0.773 ks: 0.464 | L: 0.045 auc: 0.778 ks: 0.462\n",
      "Ep   8 L: 0.045 auc: 0.789 ks: 0.432 | L: 0.045 auc: 0.773 ks: 0.483 | L: 0.045 auc: 0.778 ks: 0.449\n",
      "Ep   9 L: 0.045 auc: 0.789 ks: 0.432 | L: 0.045 auc: 0.773 ks: 0.476 | L: 0.045 auc: 0.778 ks: 0.445\n",
      "Ep  10 L: 0.045 auc: 0.789 ks: 0.435 | L: 0.045 auc: 0.773 ks: 0.477 | L: 0.045 auc: 0.778 ks: 0.446\n",
      "Ep  11 L: 0.045 auc: 0.789 ks: 0.425 | L: 0.045 auc: 0.773 ks: 0.467 | L: 0.045 auc: 0.779 ks: 0.460\n",
      "Ep  12 L: 0.045 auc: 0.789 ks: 0.433 | L: 0.045 auc: 0.773 ks: 0.484 | L: 0.045 auc: 0.779 ks: 0.449\n",
      "Ep  13 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.481 | L: 0.045 auc: 0.779 ks: 0.444\n",
      "Ep  14 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.480 | L: 0.045 auc: 0.779 ks: 0.444\n",
      "Ep  15 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.481 | L: 0.045 auc: 0.779 ks: 0.446\n",
      "Ep  16 L: 0.045 auc: 0.789 ks: 0.435 | L: 0.045 auc: 0.773 ks: 0.471 | L: 0.045 auc: 0.778 ks: 0.447\n",
      "Ep  17 L: 0.045 auc: 0.789 ks: 0.432 | L: 0.045 auc: 0.773 ks: 0.466 | L: 0.045 auc: 0.779 ks: 0.466\n",
      "Ep  18 L: 0.045 auc: 0.789 ks: 0.436 | L: 0.045 auc: 0.773 ks: 0.472 | L: 0.045 auc: 0.779 ks: 0.448\n",
      "Ep  19 L: 0.045 auc: 0.789 ks: 0.436 | L: 0.045 auc: 0.773 ks: 0.469 | L: 0.045 auc: 0.779 ks: 0.457\n",
      "Ep  20 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.475 | L: 0.045 auc: 0.779 ks: 0.464\n",
      "Ep  21 L: 0.045 auc: 0.789 ks: 0.431 | L: 0.046 auc: 0.773 ks: 0.475 | L: 0.046 auc: 0.779 ks: 0.466\n",
      "Ep  22 L: 0.045 auc: 0.789 ks: 0.435 | L: 0.046 auc: 0.773 ks: 0.469 | L: 0.046 auc: 0.779 ks: 0.447\n",
      "Ep  23 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.046 auc: 0.773 ks: 0.478 | L: 0.046 auc: 0.778 ks: 0.452\n",
      "Ep  24 L: 0.045 auc: 0.789 ks: 0.434 | L: 0.046 auc: 0.773 ks: 0.475 | L: 0.046 auc: 0.778 ks: 0.448\n",
      "Ep  25 L: 0.045 auc: 0.789 ks: 0.426 | L: 0.046 auc: 0.773 ks: 0.472 | L: 0.046 auc: 0.778 ks: 0.458\n",
      "Ep  26 L: 0.045 auc: 0.789 ks: 0.434 | L: 0.045 auc: 0.773 ks: 0.476 | L: 0.045 auc: 0.778 ks: 0.451\n",
      "Ep  27 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.046 auc: 0.773 ks: 0.477 | L: 0.046 auc: 0.778 ks: 0.450\n",
      "Ep  28 L: 0.045 auc: 0.789 ks: 0.431 | L: 0.046 auc: 0.773 ks: 0.482 | L: 0.046 auc: 0.778 ks: 0.442\n",
      "Ep  29 L: 0.045 auc: 0.789 ks: 0.433 | L: 0.045 auc: 0.773 ks: 0.466 | L: 0.045 auc: 0.778 ks: 0.466\n",
      "Ep  30 L: 0.045 auc: 0.789 ks: 0.434 | L: 0.046 auc: 0.773 ks: 0.467 | L: 0.046 auc: 0.778 ks: 0.470\n",
      "Ep  31 L: 0.045 auc: 0.789 ks: 0.435 | L: 0.045 auc: 0.773 ks: 0.468 | L: 0.045 auc: 0.778 ks: 0.469\n",
      "Ep  32 L: 0.045 auc: 0.789 ks: 0.435 | L: 0.046 auc: 0.773 ks: 0.471 | L: 0.046 auc: 0.778 ks: 0.448\n",
      "Ep  33 L: 0.045 auc: 0.789 ks: 0.431 | L: 0.045 auc: 0.773 ks: 0.482 | L: 0.045 auc: 0.778 ks: 0.448\n",
      "Ep  34 L: 0.045 auc: 0.789 ks: 0.434 | L: 0.046 auc: 0.773 ks: 0.467 | L: 0.046 auc: 0.778 ks: 0.469\n",
      "Ep  35 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.480 | L: 0.045 auc: 0.778 ks: 0.446\n",
      "Ep  36 L: 0.045 auc: 0.789 ks: 0.433 | L: 0.045 auc: 0.773 ks: 0.469 | L: 0.045 auc: 0.778 ks: 0.468\n",
      "Ep  37 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.480 | L: 0.045 auc: 0.778 ks: 0.450\n",
      "Ep  38 L: 0.045 auc: 0.789 ks: 0.434 | L: 0.046 auc: 0.773 ks: 0.468 | L: 0.046 auc: 0.778 ks: 0.457\n",
      "Ep  39 L: 0.045 auc: 0.789 ks: 0.428 | L: 0.045 auc: 0.773 ks: 0.477 | L: 0.045 auc: 0.778 ks: 0.452\n",
      "Ep  40 L: 0.045 auc: 0.789 ks: 0.430 | L: 0.045 auc: 0.773 ks: 0.480 | L: 0.045 auc: 0.778 ks: 0.450\n",
      "valid_auc 0.772758219128\n",
      "valid_ks 0.48425942256\n",
      "test_auc 0.778623712703\n",
      "test_ks 0.449017716262\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "y = data.pop('label').as_matrix()\n",
    "X = data.as_matrix()\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = dataSplit(X, y)\n",
    "\n",
    "## preprocess\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(X_train)\n",
    "\n",
    "X_train = min_max_scaler.transform(X_train)\n",
    "X_valid = min_max_scaler.transform(X_valid)\n",
    "X_test = min_max_scaler.transform(X_test)\n",
    "\n",
    "param = { 'params_model_path ': None}\n",
    "\n",
    "\n",
    "param ['params_model_path'] = 'model/Canada-A' \n",
    "if not os.path.exists(param ['params_model_path']):\n",
    "    print('training net A...')\n",
    "    training_AB(X_train, y_train, X_valid, y_valid, X_test, y_test,  0.0001, param )\n",
    "print(\"load the model A....\")\n",
    "net_A  = load_model(param ['params_model_path'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "param ['params_model_path'] = 'model/Canada-B' \n",
    "if not os.path.exists(param ['params_model_path']):\n",
    "    print('training net B...')\n",
    "    training_AB(X_train, y_train, X_valid, y_valid, X_test, y_test,  0.0002, param)\n",
    "print(\"load the model B....\")\n",
    "net_B = load_model(param ['params_model_path'])\n",
    "\n",
    "X_train_A = net_A.predict(X_train);X_train_B = net_B.predict(X_train)\n",
    "X_valid_A = net_A.predict(X_valid);X_valid_B = net_B.predict(X_valid)\n",
    "X_test_A = net_A.predict(X_test); X_test_B = net_B.predict(X_test)\n",
    "\n",
    "X_train_new = np.column_stack((X_train_A, X_train_B))\n",
    "X_valid_new = np.column_stack((X_valid_A, X_valid_B))\n",
    "X_test_new = np.column_stack((X_test_A, X_test_B))\n",
    "\n",
    "param ['params_model_path'] = 'model/Canada-C' \n",
    "print(\"runing the 2rd net...\")\n",
    "\n",
    "training_2rd(X_train_new, y_train, X_valid_new, y_valid, X_test_new, y_test, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
